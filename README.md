# ğŸ“‰ Customer Churn Prediction  

Predicting which customers are likely to leave is one of the most impactful applications of machine learning in business.  
This project applies **advanced data analysis** and **classification algorithms** to identify high-risk customers and guide effective retention strategies.  

---

## ğŸ§  Project Overview  

This notebook explores the problem of **customer churn prediction** using multiple machine learning models â€”  
**Logistic Regression**, **Random Forest**, and **XGBoost** â€” to detect which customers are most likely to leave a service.  

The project covers the complete ML workflow:
1. **Data preprocessing** and feature selection ğŸ§¹  
2. **Exploratory Data Analysis (EDA)** ğŸ”  
3. **Model training and evaluation** âš™ï¸  
4. **Comparison of predictive performance** ğŸ“Š  
5. **Churn probability estimation and business insights** ğŸ’¼  

---

## ğŸ“¦ Technologies Used  

- **Python 3**
- **Pandas**, **NumPy** â€” data handling  
- **Matplotlib**, **Seaborn** â€” visualization  
- **Scikit-learn** â€” preprocessing, model training, metrics  
- **XGBoost** â€” gradient boosting classification  

---

## âš™ï¸ Data Processing  

To enhance model performance and avoid redundancy, strongly correlated features (e.g., call minutes and number of calls) were removed.  
All features were then **standardized** to ensure that each contributes equally to the modelâ€™s learning process.

---

## ğŸ¤– Modeling & Evaluation  

Three supervised learning algorithms were trained and compared:

| Model | Key Characteristics | AUC | F1-score (Churn) | Accuracy |
|-------|---------------------|-----|------------------|-----------|
| **Logistic Regression** | Simple, interpretable baseline | ~0.88 | ~0.49 | ~0.88 |
| **Random Forest** | Ensemble, captures non-linearities | ~0.92 | ~0.82 | ~0.96 |
| **XGBoost** | Gradient boosting, best performance | **0.92** | **0.85** | **0.96** |

ğŸ“ˆ **XGBoost achieved the best balance between precision (0.93), recall (0.78), and overall accuracy (96%)**, making it the optimal choice for deployment.

---

## ğŸ§© Business Insights  

The model identified several factors strongly associated with churn:

- ğŸ“ **Frequent customer service calls** â†’ sign of dissatisfaction  
- ğŸŒ **International plan subscribers** â†’ higher churn likelihood  
- ğŸ’° **Higher total charges (especially daytime)** â†’ losing high-value clients  
- ğŸ—ºï¸ **Regional variations** â†’ some states/area codes show elevated churn  

**High-risk profile:**  
Customers with an international plan, multiple service calls, and higher total charges â€” especially in certain regions â€” are significantly more likely to leave.

---

## ğŸ¯ Strategy Optimization  

Using the XGBoost model, the top 500 customers with the highest churn probabilities were selected for targeted retention.  

- Among these **top 500**, **42.2% actually churned**, compared to **14.1% Â± 2.9%** in random outreach.  
- This means the model-based strategy is **3Ã— more effective** than random selection.  


---

## ğŸ’¡ Key Takeaways  

- Predictive analytics can dramatically improve **customer retention strategies**.  
- **XGBoost** provided the best trade-off between interpretability and predictive power.  
- The approach enables **efficient resource allocation** â€” focusing on the customers who matter most.  


